{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bkvhp-ejtvOf"
   },
   "source": [
    "Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJwDh_fTR_8h",
    "outputId": "803de422-f6ae-4b52-b5e5-eadbd4fe6234"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFYEDQrXt4MZ"
   },
   "source": [
    "Imports & Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ny-mvqoSpm1",
    "outputId": "15ca24f5-4a09-4e69-f2b0-ba9335b53ff2"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --no-deps --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMkFuHkUn3Hd"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSBG4LLxVnA3"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4 --force-reinstall --no-cache-dir\n",
    "!pip install numpy==1.26.4 -q --disable-pip-version-check\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MukMURFXVnFS"
   },
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "transformers==4.41.2 \\\n",
    "tokenizers==0.19.1 \\\n",
    "sentence-transformers==2.7.0 \\\n",
    "peft==0.10.0 \\\n",
    "accelerate==0.30.1 \\\n",
    "faiss-cpu==1.8.0 \\\n",
    "pypdf==4.2.0 \\\n",
    "langchain==0.2.6 \\\n",
    "langchain-community==0.2.6 \\\n",
    "langchain-text-splitters==0.2.1 \\\n",
    "google-generativeai==0.5.4 \\\n",
    "requests==2.32.3 \\\n",
    "triton==2.3.1 \\\n",
    "--no-cache-dir -q --disable-pip-version-check 2>/dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RbDj6phQT2Gq",
    "outputId": "d42e18ef-24dd-478b-e714-18c9ed0d1b0d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import transformers\n",
    "import peft\n",
    "import faiss\n",
    "import sentence_transformers\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"NumPy:\", numpy.__version__)\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"PEFT:\", peft.__version__)\n",
    "print(\"FAISS OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOcLkQwwU2XG",
    "outputId": "eeef75b7-bc67-4a08-df46-a91cfc4578c0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    InputExample,\n",
    "    losses\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDfNX7MEWa_K"
   },
   "source": [
    "PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJmciwmwWRmb"
   },
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        extracted = page.extract_text()\n",
    "        if extracted:\n",
    "            text += extracted + \"\\n\"\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD-IcCJtuFN3"
   },
   "source": [
    "Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyFJJ0lIWZ49"
   },
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    return splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtslbkWDuHzE"
   },
   "source": [
    "Fine-Tune BERT (Sentence Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vltvvtvMWfCk"
   },
   "outputs": [],
   "source": [
    "train_examples = [\n",
    "    InputExample(\n",
    "        texts=[\n",
    "            \"This paper proposes a transformer-based NLP model\",\n",
    "            \"We introduce a transformer architecture for language understanding\"\n",
    "        ],\n",
    "        label=0.9\n",
    "    ),\n",
    "    InputExample(\n",
    "        texts=[\n",
    "            \"The dataset is collected from medical imaging\",\n",
    "            \"This work focuses on reinforcement learning agents\"\n",
    "        ],\n",
    "        label=0.2\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0eyqANKuKy7"
   },
   "source": [
    "Train BERT with Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "52c9b46b43234479ae3355f7a557255d",
      "e2938dbeb6a24a54ae1e71cb19c766ea",
      "a305bb1dafab4f53b27f274b8e24db12",
      "267ee44b975f43568379214902d9d2a5",
      "105ab0c43c914a42a6736164c71982ab",
      "93285892764f4ee2ab4f88b7d1ae7af9",
      "a379afe78a0f4f739add41ca5ed2317c",
      "2a233be392c14757836fe9e1d0c0f360",
      "e59dec28dd894242b6e1b7aaf72511b1",
      "3565474c0443497291f782894687631e",
      "f284a2d3da4649ccaa091956eb8c0be1",
      "636a3e837c374c14b2ea33c6ed2ff683",
      "d9791b6dd32f4aabb1184100dab8d33a",
      "f186824550ae4097821a7dc90dd884a3",
      "aba902a6c124425da3906d0d3869ec08",
      "e561402113064e958a61984b62654e23",
      "03a1d2bfd46141d3ab7cd43ccf9fdcef",
      "5c7e24ef5cd040a28109be9ddb823c05",
      "c5f5d5b1d0704ebda7634e7c78cb767d",
      "9494dbb41c3b45ea939b90ca919a2de2",
      "520f078a1d334cab938c8c11b2cce27b",
      "2a9b740f8c294fc5a0f76f7087b9e8e9",
      "b3e84dff6d9b40b58f4570acd96777a9",
      "8c37952533c44325af98f84343bb10b5",
      "00dc53e54ae5456298b662bdca804e49",
      "22592bc2570141ecbbf70a3845fc9f22",
      "dd4f391531614428964f33127d30c9ff",
      "b1ac04bd007c4bf0973eae91e074d56c",
      "f312e1ab642c4bf6a9e7af02510ac03a",
      "089691c2c11444c282305238e80ea349",
      "fbff3c66325442e0a436bebc8ac146aa",
      "da5c1fece11843858385394f2b991058",
      "1d841c39b72a47a9bd89da8f464a2228"
     ]
    },
    "id": "u8PtP7t_Wg06",
    "outputId": "687765fc-cb49-40aa-9aa8-97407684c12c"
   },
   "outputs": [],
   "source": [
    "bert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_examples,\n",
    "    shuffle=True,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "loss_fn = losses.CosineSimilarityLoss(bert_model)\n",
    "\n",
    "bert_model.fit(\n",
    "    train_objectives=[(train_loader, loss_fn)],\n",
    "    epochs=2,\n",
    "    warmup_steps=100\n",
    ")\n",
    "\n",
    "bert_model.save(\"bert_pdf_similarity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENgAfXCHuPct"
   },
   "source": [
    "\n",
    "Load Fine-Tuned BERT + FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7L_AmrSDWizb"
   },
   "outputs": [],
   "source": [
    "# bert_embedder = SentenceTransformer(\"bert_pdf_similarity\")\n",
    "\n",
    "# dimension = 768\n",
    "# faiss_index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# stored_chunks = []\n",
    "\n",
    "bert_embedder = SentenceTransformer(\"bert_pdf_similarity\")\n",
    "\n",
    "dimension = bert_embedder.get_sentence_embedding_dimension()\n",
    "faiss_index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "stored_chunks = []  # ‚úÖ REQUIRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZiJdGf_uR27"
   },
   "source": [
    "Store PDF Embeddings in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92BaagOgWmqe"
   },
   "outputs": [],
   "source": [
    "# def store_pdf_chunks(chunks):\n",
    "#     embeddings = bert_embedder.encode(\n",
    "#         chunks,\n",
    "#         convert_to_numpy=True,\n",
    "#         normalize_embeddings=True\n",
    "#     )\n",
    "#     faiss_index.add(embeddings)\n",
    "#     stored_chunks.extend(chunks)\n",
    "\n",
    "def store_pdf_chunks(chunks):\n",
    "    embeddings = bert_embedder.encode(\n",
    "        chunks,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    # ensure 2D shape\n",
    "    if embeddings.ndim == 1:\n",
    "        embeddings = embeddings.reshape(1, -1)\n",
    "\n",
    "    faiss_index.add(embeddings)\n",
    "    stored_chunks.extend(chunks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuV8joBeuU8s"
   },
   "source": [
    "Cross-PDF Similarity (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOm4bKjPWopJ"
   },
   "outputs": [],
   "source": [
    "def compute_cross_pdf_similarity(chunks):\n",
    "    embeddings = bert_embedder.encode(\n",
    "        chunks,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    scores, _ = faiss_index.search(embeddings, k=5)\n",
    "    similarity_percent = scores.mean() * 100\n",
    "\n",
    "    return round(float(similarity_percent), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5zRE_bCuX54"
   },
   "source": [
    "Load LLaMA/ Mistral (Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBC7g2ahWqrh"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import torch\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"  # or TinyLlama\n",
    "# model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4ASW3buuanH"
   },
   "source": [
    "Apply LoRA (Efficient Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNZd_eb3Wsg3",
    "outputId": "b1277a33-8728-47a4-c531-92a22a537426"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uIutrP2ub_u"
   },
   "source": [
    "Structured Summarization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3DocYKGtW0hm"
   },
   "outputs": [],
   "source": [
    "def summarize_with_mistral(text):\n",
    "    prompt = f\"\"\"[INST]\n",
    "You are an academic research assistant.\n",
    "Summarize the paper in STRICT structured format:\n",
    "\n",
    "### ANALYSIS:\n",
    "- Objective\n",
    "- Methodology\n",
    "- Findings\n",
    "- Limitations\n",
    "\n",
    "### SUMMARY:\n",
    "- Short academic explanation of the paper\n",
    "\n",
    "Paper:\n",
    "{text[:3000]}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=400,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        output[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTqtoBWTjiIG"
   },
   "outputs": [],
   "source": [
    "# def format_for_presenter(mistral_output):\n",
    "#     analysis = \"\"\n",
    "#     summary = \"\"\n",
    "\n",
    "#     if \"### ANALYSIS:\" in mistral_output:\n",
    "#         analysis = mistral_output.split(\"### SUMMARY:\")[0].replace(\"### ANALYSIS:\", \"\").strip()\n",
    "\n",
    "#     if \"### SUMMARY:\" in mistral_output:\n",
    "#         summary = mistral_output.split(\"### SUMMARY:\")[1].strip()\n",
    "\n",
    "#     presenter = PresenterAgent()\n",
    "#     return presenter.run({\n",
    "#         \"analysis\": analysis,\n",
    "#         \"summary\": summary\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXI4zaTkulp3"
   },
   "source": [
    "Gemini Chat Setup (PDF-Scoped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c73DkLYQW2jE"
   },
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "gemini = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "pdf_chat_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bmND6cuunop"
   },
   "source": [
    "Chat with PDF Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pToKvpoyW41U"
   },
   "outputs": [],
   "source": [
    "# def chat_with_pdf(question, summary):\n",
    "#     prompt = f\"\"\"\n",
    "# Context (PDF Summary):\n",
    "# {summary}\n",
    "\n",
    "# User Question:\n",
    "# {question}\n",
    "# \"\"\"\n",
    "#     response = gemini.generate_content(prompt)\n",
    "#     return response.text\n",
    "\n",
    "\n",
    "def chat_with_pdf(question, context):\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Use the context below to answer the question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer clearly and concisely.\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE7HaZDkurzN"
   },
   "source": [
    "Full Pipeline (End-to-End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SClSbnfCW6fS"
   },
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path):\n",
    "    text = load_pdf_text(pdf_path)\n",
    "    chunks = split_text(text)\n",
    "\n",
    "    store_pdf_chunks(chunks)\n",
    "\n",
    "    similarity = compute_cross_pdf_similarity(chunks)\n",
    "    summary = summarize_with_mistral(text)\n",
    "\n",
    "    return {\n",
    "        \"cross_pdf_similarity_percent\": similarity,\n",
    "        \"summary\": summary\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVCFCVm1W8Du"
   },
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path):\n",
    "    print(\"üìÑ Loading PDF...\")\n",
    "    text = load_pdf_text(pdf_path)\n",
    "\n",
    "    print(\"‚úÇÔ∏è Splitting text...\")\n",
    "    chunks = split_text(text)\n",
    "\n",
    "    print(\"üì¶ Storing chunks...\")\n",
    "    store_pdf_chunks(chunks)\n",
    "\n",
    "    print(\"üîç Computing similarity...\")\n",
    "    similarity = compute_cross_pdf_similarity(chunks)\n",
    "\n",
    "    print(\"üß† Generating summary (this is slow)...\")\n",
    "    summary = summarize_with_mistral(text)\n",
    "\n",
    "    print(\"‚úÖ Done\")\n",
    "    return {\n",
    "        \"cross_pdf_similarity_percent\": similarity,\n",
    "        \"summary\": summary\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba5-u0SuuzBX"
   },
   "source": [
    "Run Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPnHbufqW959",
    "outputId": "30f48c21-55b9-4e94-f69e-366ba18df759"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "result = process_pdf(\"Sample1.pdf\")\n",
    "\n",
    "print(f\"üîπ Cross-PDF Similarity (%): {result['cross_pdf_similarity_percent']:.2f}\")\n",
    "print(\"\\nüîπ Structured Summary:\\n\")\n",
    "print(result[\"summary\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7KWLG2oYAwp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
<<<<<<< HEAD
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAZjLNR_4mxxxxxxxxxxxxxxxxxxxxxx\"\n",
=======
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyAZjLNR_4mjhtxxxxxxxxxxxxxxxxx\"\n",
>>>>>>> 82fa438d2a7e75a368e2cf0d7fc61c31597b1c15
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "gemini = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "pdf_chat_memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAyHuL13u6oG"
   },
   "source": [
    "Chat Below Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRd74S__ZDwN"
   },
   "outputs": [],
   "source": [
    "# question = \"What is the main contribution of this paper?\"\n",
    "# answer = chat_with_pdf(question, result[\"summary\"])\n",
    "# print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b8826d8"
   },
   "source": [
    "### Summarization Accuracy Evaluation\n",
    "\n",
    "To evaluate summarization accuracy, we need a reference (ground truth) summary. For demonstration purposes, let's define a hypothetical reference summary. We will then use cosine similarity between the embeddings of the generated summary and the reference summary to get a basic 'accuracy' score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42cec571",
    "outputId": "c74077aa-2f34-402e-ae95-727c94abe975"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define a hypothetical reference summary for the 'Sample1.pdf'\n",
    "# In a real-world scenario, this would be a human-written summary.\n",
    "reference_summary = \"The paper investigates how advanced English learners use mobile devices for language learning. The study involved 20 students, using semi-structured interviews. Data analysis, both qualitative and quantitative, revealed that while some students were highly aware of mobile devices' benefits and used them effectively for self-directed learning, others used them more intuitively or ad hoc in the classroom. Key themes include learner autonomy, mobile devices, and advanced EFL learners.\"\n",
    "\n",
    "# Get the generated summary from the previous `process_pdf` call\n",
    "generated_summary = result[\"summary\"]\n",
    "\n",
    "# Encode both summaries using the bert_embedder\n",
    "reference_embedding = bert_embedder.encode([reference_summary], convert_to_numpy=True, normalize_embeddings=True)\n",
    "generated_embedding = bert_embedder.encode([generated_summary], convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = cosine_similarity(reference_embedding, generated_embedding)[0][0]\n",
    "\n",
    "print(f\"Reference Summary:\\n{reference_summary}\\n\")\n",
    "print(f\"Generated Summary:\\n{generated_summary}\\n\")\n",
    "print(f\"Cosine Similarity (Generated vs. Reference): {similarity_score:.4f}\")\n",
    "\n",
    "# Interpretation (for demonstration)\n",
    "if similarity_score > 0.8:\n",
    "    print(\"This is a high similarity score, suggesting the generated summary is quite similar to the reference.\")\n",
    "elif similarity_score > 0.5:\n",
    "    print(\"This is a moderate similarity score.\")\n",
    "else:\n",
    "    print(\"This is a low similarity score, indicating less similarity to the reference.\")\n",
    "\n",
    "print(\"\\nNote: This is a basic similarity measure. For robust summarization evaluation, metrics like ROUGE scores are typically used.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
